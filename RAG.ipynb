{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Getting Started\n",
        "Retrieval-Augmented Generation (RAG) is a powerful technique for making Large Language Models smarter and more trustworthy. It works by \"grounding\" the model, forcing it to base its answers on specific, retrieved information rather than just its internal knowledge. This tutorial will guide you step-by-step through building your own RAG pipeline. We will use a vector database for efficient retrieval and the powerful Gemini API for the final, context-aware generation step.\n",
        "\n",
        "Credits : [Willy Zhuang](https://medium.com/data-on-cloud-genai-data-science-and-data/building-a-rag-system-with-gemini-and-chromadb-6ead6452bcf5)'s Medium post"
      ],
      "metadata": {
        "id": "0bchWm4u6Ooy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzcw6rEH6D4y",
        "outputId": "47f3748f-97c6-4952-9e40-1f66dd4c02e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/310.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m307.2/310.5 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m310.5/310.5 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install --quiet langchain chromadb langchain-google-genai langchain_community pypdf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using Gemini API KEY\n",
        "Here's how to set your Gemini API key in Colab secrets:\n",
        "Click the key icon (ðŸ”‘) in the left sidebar to open the \"Secrets\" tab.\n",
        "Enter a name for your secret (e.g., GOOGLE_API_KEY), paste your API key into the \"Value\" field, and make sure the \"Notebook access\" toggle is turned on."
      ],
      "metadata": {
        "id": "HDjOQLyaFFy6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "\n",
        "# Get the API key from Colab secrets\n",
        "API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Configure the genai library with the API key\n",
        "genai.configure(api_key=API_KEY)"
      ],
      "metadata": {
        "id": "nMjOy3wp6qko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the 'auth' module from the 'google.colab' library.\n",
        "# This module provides the necessary functions to authenticate the user\n",
        "# running the notebook.\n",
        "from google.colab import auth\n",
        "# Call the 'authenticate_user' function.\n",
        "# When this line is executed in a Colab notebook, it will trigger\n",
        "# a pop-up window. This window prompts the user to go to a URL,\n",
        "# log in with their Google account, and paste an authorization code back.\n",
        "# This process grants the notebook credentials to access other\n",
        "# Google Cloud services (like Google Drive or BigQuery) on the user's behalf.\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "Mh9wWwbl68Kz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Gemini API KEY"
      ],
      "metadata": {
        "id": "huSWAYVgGUSV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", google_api_key=API_KEY)\n",
        "try:\n",
        "    response = llm.invoke(\"What is LLM?\")\n",
        "    print(response)\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSEACh_U7BNc",
        "outputId": "9ac86c32-4911-4d07-fb9e-a5443033d803"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='**LLM stands for Large Language Model.**\\n\\nIn simple terms, an LLM is a type of **artificial intelligence (AI) model** that is designed to **understand, generate, and process human-like text**.\\n\\nHere\\'s a breakdown of what that means:\\n\\n1.  **Large:**\\n    *   **Parameters:** These models are \"large\" because they contain billions, or even trillions, of parameters. Parameters are the values (like weights and biases in a neural network) that the model learns during training, essentially representing its knowledge and understanding. More parameters generally allow for more complex learning and better performance.\\n    *   **Training Data:** They are trained on truly massive datasets of text and code from the internet (books, articles, websites, conversations, code repositories, etc.). This vast amount of data is crucial for them to learn the nuances of language, facts, reasoning, and various writing styles.\\n\\n2.  **Language:**\\n    *   Their primary domain is **human language**. They learn the statistical relationships between words, phrases, and concepts. They understand grammar, syntax, semantics, and even pragmatics (the use of language in context).\\n\\n3.  **Model:**\\n    *   They are a specific type of **deep learning model**, usually based on a neural network architecture called a \"Transformer.\" This architecture is particularly good at processing sequences of data, like words in a sentence.\\n    *   During training, the model learns to predict the next word in a sequence, given the preceding words. This seemingly simple task, performed over massive datasets, allows it to develop a profound understanding of how language works.\\n\\n**What LLMs Can Do (Capabilities):**\\n\\nBecause of their extensive training, LLMs are incredibly versatile and can perform a wide range of language-related tasks:\\n\\n*   **Answering Questions:** Providing informative answers to queries.\\n*   **Generating Text:** Writing essays, articles, stories, poems, emails, code, scripts, etc.\\n*   **Summarization:** Condensing long texts into shorter, coherent summaries.\\n*   **Translation:** Translating text from one language to another.\\n*   **Chatbots & Conversational AI:** Engaging in natural, human-like conversations.\\n*   **Code Generation & Debugging:** Writing code, suggesting improvements, or finding errors.\\n*   **Brainstorming & Idea Generation:** Helping users explore new concepts or ideas.\\n*   **Sentiment Analysis:** Determining the emotional tone of a piece of text.\\n*   **Information Extraction:** Pulling specific data points from unstructured text.\\n\\n**Examples of LLMs:**\\n\\nSome well-known examples of LLMs include:\\n\\n*   **GPT-3, GPT-3.5, GPT-4** (from OpenAI, powering ChatGPT)\\n*   **Gemini** (from Google)\\n*   **Claude** (from Anthropic)\\n*   **Llama** (from Meta)\\n\\n**Key Takeaway:**\\n\\nAn LLM is essentially a highly sophisticated text prediction machine, trained on an enormous amount of human language data. It doesn\\'t \"understand\" or \"think\" in the human sense, but it\\'s incredibly good at recognizing patterns and generating text that *appears* to be intelligent and coherent, making it a powerful tool for a multitude of applications.' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []} id='run--f572d4b2-0ee9-4c97-a61e-117ce0b05124-0' usage_metadata={'input_tokens': 6, 'output_tokens': 1807, 'total_tokens': 1813, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 1118}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Required libraries"
      ],
      "metadata": {
        "id": "0m6a3N5-GmVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "from langchain import hub\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.document_loaders import WebBaseLoader\n",
        "from langchain.schema import StrOutputParser\n",
        "from langchain.schema.prompt_template import format_document\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.vectorstores import Chroma\n",
        "import requests\n",
        "from pypdf import PdfReader\n",
        "import os\n",
        "import re\n",
        "from typing import List\n",
        "import google.generativeai as genai\n",
        "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
        "import chromadb"
      ],
      "metadata": {
        "id": "mSvMezwG7HIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download a Document  "
      ],
      "metadata": {
        "id": "uuzIDJLFIA3L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://services.google.com/fh/files/misc/ai_adoption_framework_whitepaper.pdf\"\n",
        "save_path =\"white_paper.pdf\"\n",
        "\n",
        "#save it locally\n",
        "response = requests.get(url)\n",
        "with open(save_path, 'wb') as f:\n",
        "    f.write(response.content)"
      ],
      "metadata": {
        "id": "L_SSYF9q8Xe7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To use your own document replace the document path\n",
        "reader = PdfReader(save_path)"
      ],
      "metadata": {
        "id": "MSJWlLq7IWxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Document parsing"
      ],
      "metadata": {
        "id": "xPhgHW4MIrNE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\n",
        "for page in reader.pages:\n",
        "    page_text = page.extract_text()\n",
        "    if page_text:\n",
        "        text += page_text"
      ],
      "metadata": {
        "id": "DU9hJuAQ91HD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chuncking"
      ],
      "metadata": {
        "id": "awYV7GxLIyTN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the text into chunks based on double newlines\n",
        "def split_text(text):\n",
        "    return [i for i in re.split('\\n\\n', text) if i.strip()]\n"
      ],
      "metadata": {
        "id": "_242B7mj-i5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunked_text = split_text(text)"
      ],
      "metadata": {
        "id": "pZqWhfhY-q5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating Embeddings"
      ],
      "metadata": {
        "id": "21bMIXcPI2Ma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a custom embedding function using Gemini API\n",
        "class GeminiEmbeddingFunction(EmbeddingFunction):\n",
        "    def __call__(self, input: Documents) -> Embeddings:\n",
        "        genai.configure(api_key=API_KEY)\n",
        "        model = \"gemini-embedding-001\"\n",
        "        title = \"Custom query\"\n",
        "        return genai.embed_content(model=model, content=input, task_type=\"retrieval_document\", title=title)[\"embedding\"]"
      ],
      "metadata": {
        "id": "gok5Ujwt-0u0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create directory for database if it doesn't exist\n",
        "db_folder = \"chroma_db\"\n",
        "if not os.path.exists(db_folder):\n",
        "    os.makedirs(db_folder)\n"
      ],
      "metadata": {
        "id": "BwFKsSKK_ySC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a vector store"
      ],
      "metadata": {
        "id": "mOdIn4CMJCGw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Chroma database with the given documents\n",
        "import time\n",
        "\n",
        "def create_chroma_db(documents: List[str], path: str, name: str):\n",
        "    chroma_client = chromadb.PersistentClient(path=path)\n",
        "    # Attempt to get the collection, if it exists\n",
        "    try:\n",
        "        db = chroma_client.get_collection(name=name, embedding_function=GeminiEmbeddingFunction())\n",
        "        print(f\"Collection '{name}' already exists. Using existing collection.\")\n",
        "    except: # If the collection does not exist, create it\n",
        "        db = chroma_client.create_collection(name=name, embedding_function=GeminiEmbeddingFunction())\n",
        "        print(f\"Collection '{name}' created.\")\n",
        "\n",
        "    # Add documents to the collection with a delay\n",
        "    for i, d in enumerate(documents):\n",
        "        try:\n",
        "            db.add(documents=[d], ids=[str(i)])\n",
        "            print(f\"Added document {i+1}/{len(documents)}\")\n",
        "            time.sleep(1) # Add a 1-second delay\n",
        "        except Exception as e:\n",
        "            print(f\"Error adding document {i+1}: {e}\")\n",
        "            # You might want to add more sophisticated error handling here,\n",
        "            # like retrying or logging the error.\n",
        "\n",
        "    return db, name"
      ],
      "metadata": {
        "id": "qWm4ulGKAEsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the path and collection name for Chroma database\n",
        "db_name = \"rag_experiment\"\n",
        "db_path = os.path.join(os.getcwd(), db_folder)\n",
        "db, db_name = create_chroma_db(chunked_text, db_path, db_name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "SAHinkMYAHwk",
        "outputId": "7024b146-4ebb-4a76-9a2a-79176101bdbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2208356782.py:8: DeprecationWarning: The class GeminiEmbeddingFunction does not implement __init__. This will be required in a future version.\n",
            "  db = chroma_client.get_collection(name=name, embedding_function=GeminiEmbeddingFunction())\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collection 'rag_experiment' already exists. Using existing collection.\n",
            "Added document 1/1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load an existing Chroma collection\n",
        "def load_chroma_collection(path: str, name: str):\n",
        "    chroma_client = chromadb.PersistentClient(path=path)\n",
        "    return chroma_client.get_collection(name=name, embedding_function=GeminiEmbeddingFunction())\n"
      ],
      "metadata": {
        "id": "6hPp5hzuATEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db = load_chroma_collection(db_path, db_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H67mGvIFCaGF",
        "outputId": "89823798-b4c3-40ba-9253-ebfa2070ae07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-830655917.py:4: DeprecationWarning: The class GeminiEmbeddingFunction does not implement __init__. This will be required in a future version.\n",
            "  return chroma_client.get_collection(name=name, embedding_function=GeminiEmbeddingFunction())\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Retrieve the context"
      ],
      "metadata": {
        "id": "U5nmHkKtJIQY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve the most relevant passages based on the query\n",
        "def get_relevant_passage(query: str, db, n_results: int):\n",
        "    results = db.query(query_texts=[query], n_results=n_results)\n",
        "    return [doc[0] for doc in results['documents']]\n"
      ],
      "metadata": {
        "id": "QZgAkHnSCdyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is the AI Maturity Scale?\"\n",
        "relevant_text = get_relevant_passage(query, db, n_results=1)\n"
      ],
      "metadata": {
        "id": "vF80bJ51CjcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Answer Generation"
      ],
      "metadata": {
        "id": "8eZBcrNhJTNe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct a prompt for the generation model based on the query and retrieved data\n",
        "def make_rag_prompt(query: str, relevant_passage: str):\n",
        "    escaped_passage = relevant_passage.replace(\"'\", \"\").replace('\"', \"\").replace(\"\\n\", \" \")\n",
        "    prompt = f\"\"\"You are a helpful and informative bot that answers questions using text from the reference passage included below.\n",
        "Be sure to respond in a complete sentence, being comprehensive, including all relevant background information.\n",
        "However, you are talking to a non-technical audience, so be sure to break down complicated concepts and\n",
        "strike a friendly and conversational tone.\n",
        "QUESTION: '{query}'\n",
        "PASSAGE: '{escaped_passage}'\n",
        "\n",
        "ANSWER:\n",
        "\"\"\"\n",
        "    return prompt\n"
      ],
      "metadata": {
        "id": "gHIcTcG7CnCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate an answer using the Gemini Pro API\n",
        "def generate_answer(prompt: str):\n",
        "\n",
        "    genai.configure(api_key=API_KEY)\n",
        "    model = genai.GenerativeModel('gemini-2.5-flash')\n",
        "    result = model.generate_content(prompt)\n",
        "    return result.text"
      ],
      "metadata": {
        "id": "6aO2CCYUCuRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct the prompt and generate the answer\n",
        "final_prompt = make_rag_prompt(query, \"\".join(relevant_text))\n",
        "answer = generate_answer(final_prompt)\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "0io1DrGGC170",
        "outputId": "fd90be9f-c214-42fc-b58f-2a5c450c6ad7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The AI Maturity Scale is a helpful tool that combines different aspects of how an organization uses Artificial Intelligence, called \"themes,\" with various stages of progress, known as \"phases,\" to show a complete picture of an organization's AI journey. Essentially, it's a guide that evaluates six key themesâ€”like how well your team learns and leads AI initiatives, how easily they can access and share data, how effectively they can scale AI projects, how securely they protect information, and how much they automate AI processesâ€”and then places your organization's practices within one of three phases: \"Tactical\" (just starting out with simple, short-term uses), \"Strategic\" (using AI for sustainable business value with a broader vision), or \"Transformational\" (where AI plays a central role in driving innovation and continuous learning). This allows organizations to understand where they are currently in their AI adoption and helps them map out the necessary steps to evolve and build more powerful AI capabilities.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing"
      ],
      "metadata": {
        "id": "AshUF7ZMJab9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Interactive function to process user input and generate an answer\n",
        "def process_query_and_generate_answer():\n",
        "    query = input(\"Please enter your query: \")\n",
        "    if not query:\n",
        "        print(\"No query provided.\")\n",
        "        return\n",
        "    db = load_chroma_collection(db_path, db_name)\n",
        "    relevant_text = get_relevant_passage(query, db, n_results=1)\n",
        "    if not relevant_text:\n",
        "        print(\"No relevant information found for the given query.\")\n",
        "        return\n",
        "    final_prompt = make_rag_prompt(query, \"\".join(relevant_text))\n",
        "    answer = generate_answer(final_prompt)\n",
        "    print(\"Generated Answer:\", answer)\n"
      ],
      "metadata": {
        "id": "Vmlxm0hsDH0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Invoke the function to interact with user\n",
        "process_query_and_generate_answer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "9NXv6ZltDbsZ",
        "outputId": "b16d2d7e-5fb1-4f76-df0d-b6babc6752c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please enter your query: what is AI\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-830655917.py:4: DeprecationWarning: The class GeminiEmbeddingFunction does not implement __init__. This will be required in a future version.\n",
            "  return chroma_client.get_collection(name=name, embedding_function=GeminiEmbeddingFunction())\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Answer: Artificial Intelligence, or AI, is all about the theory and development of computer systems that can perform tasks we usually think require human intelligence! Think of things like recognizing faces and objects (visual perception), understanding what we say (speech recognition), or even making smart choices (decision-making). The passage also tells us that Machine Learning, or ML, is a really effective way to build these AI systems by teaching them to find useful patterns in data all by themselves, instead of us having to give them a long list of specific rules to follow.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FUxGiOLZDeDq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}