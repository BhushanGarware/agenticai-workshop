{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Getting Started\n",
        "Retrieval-Augmented Generation (RAG) is a powerful technique for making Large Language Models smarter and more trustworthy. It works by \"grounding\" the model, forcing it to base its answers on specific, retrieved information rather than just its internal knowledge. This tutorial will guide you step-by-step through building your own RAG pipeline. We will use a vector database for efficient retrieval and the powerful Gemini API for the final, context-aware generation step.\n",
        "\n",
        "Credits : [Willy Zhuang](https://medium.com/data-on-cloud-genai-data-science-and-data/building-a-rag-system-with-gemini-and-chromadb-6ead6452bcf5)'s Medium post"
      ],
      "metadata": {
        "id": "0bchWm4u6Ooy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzcw6rEH6D4y"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet langchain chromadb langchain-google-genai langchain_community pypdf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using Gemini API KEY\n",
        "Here's how to set your Gemini API key in Colab secrets:\n",
        "Click the key icon (ðŸ”‘) in the left sidebar to open the \"Secrets\" tab.\n",
        "Enter a name for your secret (e.g., GOOGLE_API_KEY), paste your API key into the \"Value\" field, and make sure the \"Notebook access\" toggle is turned on."
      ],
      "metadata": {
        "id": "HDjOQLyaFFy6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "\n",
        "# Get the API key from Colab secrets\n",
        "API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Configure the genai library with the API key\n",
        "genai.configure(api_key=API_KEY)"
      ],
      "metadata": {
        "id": "nMjOy3wp6qko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the 'auth' module from the 'google.colab' library.\n",
        "# This module provides the necessary functions to authenticate the user\n",
        "# running the notebook.\n",
        "from google.colab import auth\n",
        "# Call the 'authenticate_user' function.\n",
        "# When this line is executed in a Colab notebook, it will trigger\n",
        "# a pop-up window. This window prompts the user to go to a URL,\n",
        "# log in with their Google account, and paste an authorization code back.\n",
        "# This process grants the notebook credentials to access other\n",
        "# Google Cloud services (like Google Drive or BigQuery) on the user's behalf.\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "Mh9wWwbl68Kz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Gemini API KEY"
      ],
      "metadata": {
        "id": "huSWAYVgGUSV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", google_api_key=API_KEY)\n",
        "try:\n",
        "    response = llm.invoke(\"What is LLM?\")\n",
        "    print(response)\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n"
      ],
      "metadata": {
        "id": "jSEACh_U7BNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Required libraries"
      ],
      "metadata": {
        "id": "0m6a3N5-GmVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "from langchain import hub\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.document_loaders import WebBaseLoader\n",
        "from langchain.schema import StrOutputParser\n",
        "from langchain.schema.prompt_template import format_document\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.vectorstores import Chroma\n",
        "import requests\n",
        "from pypdf import PdfReader\n",
        "import os\n",
        "import re\n",
        "from typing import List\n",
        "import google.generativeai as genai\n",
        "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
        "import chromadb"
      ],
      "metadata": {
        "id": "mSvMezwG7HIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download a Document  "
      ],
      "metadata": {
        "id": "uuzIDJLFIA3L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://services.google.com/fh/files/misc/ai_adoption_framework_whitepaper.pdf\"\n",
        "save_path =\"white_paper.pdf\"\n",
        "\n",
        "#save it locally\n",
        "response = requests.get(url)\n",
        "with open(save_path, 'wb') as f:\n",
        "    f.write(response.content)"
      ],
      "metadata": {
        "id": "L_SSYF9q8Xe7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To use your own document replace the document path\n",
        "reader = PdfReader(save_path)"
      ],
      "metadata": {
        "id": "MSJWlLq7IWxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Document parsing"
      ],
      "metadata": {
        "id": "xPhgHW4MIrNE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\n",
        "for page in reader.pages:\n",
        "    page_text = page.extract_text()\n",
        "    if page_text:\n",
        "        text += page_text"
      ],
      "metadata": {
        "id": "DU9hJuAQ91HD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chuncking"
      ],
      "metadata": {
        "id": "awYV7GxLIyTN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the text into chunks based on double newlines\n",
        "def split_text(text):\n",
        "    return [i for i in re.split('\\n\\n', text) if i.strip()]\n"
      ],
      "metadata": {
        "id": "_242B7mj-i5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunked_text = split_text(text)"
      ],
      "metadata": {
        "id": "pZqWhfhY-q5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating Embeddings"
      ],
      "metadata": {
        "id": "21bMIXcPI2Ma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a custom embedding function using Gemini API\n",
        "class GeminiEmbeddingFunction(EmbeddingFunction):\n",
        "    def __call__(self, input: Documents) -> Embeddings:\n",
        "        genai.configure(api_key=API_KEY)\n",
        "        model = \"gemini-embedding-001\"\n",
        "        title = \"Custom query\"\n",
        "        return genai.embed_content(model=model, content=input, task_type=\"retrieval_document\", title=title)[\"embedding\"]"
      ],
      "metadata": {
        "id": "gok5Ujwt-0u0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create directory for database if it doesn't exist\n",
        "db_folder = \"chroma_db\"\n",
        "if not os.path.exists(db_folder):\n",
        "    os.makedirs(db_folder)\n"
      ],
      "metadata": {
        "id": "BwFKsSKK_ySC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a vector store"
      ],
      "metadata": {
        "id": "mOdIn4CMJCGw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Chroma database with the given documents\n",
        "import time\n",
        "\n",
        "def create_chroma_db(documents: List[str], path: str, name: str):\n",
        "    chroma_client = chromadb.PersistentClient(path=path)\n",
        "    # Attempt to get the collection, if it exists\n",
        "    try:\n",
        "        db = chroma_client.get_collection(name=name, embedding_function=GeminiEmbeddingFunction())\n",
        "        print(f\"Collection '{name}' already exists. Using existing collection.\")\n",
        "    except: # If the collection does not exist, create it\n",
        "        db = chroma_client.create_collection(name=name, embedding_function=GeminiEmbeddingFunction())\n",
        "        print(f\"Collection '{name}' created.\")\n",
        "\n",
        "    # Add documents to the collection with a delay\n",
        "    for i, d in enumerate(documents):\n",
        "        try:\n",
        "            db.add(documents=[d], ids=[str(i)])\n",
        "            print(f\"Added document {i+1}/{len(documents)}\")\n",
        "            time.sleep(1) # Add a 1-second delay\n",
        "        except Exception as e:\n",
        "            print(f\"Error adding document {i+1}: {e}\")\n",
        "            # You might want to add more sophisticated error handling here,\n",
        "            # like retrying or logging the error.\n",
        "\n",
        "    return db, name"
      ],
      "metadata": {
        "id": "qWm4ulGKAEsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the path and collection name for Chroma database\n",
        "db_name = \"rag_experiment\"\n",
        "db_path = os.path.join(os.getcwd(), db_folder)\n",
        "db, db_name = create_chroma_db(chunked_text, db_path, db_name)\n"
      ],
      "metadata": {
        "id": "SAHinkMYAHwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load an existing Chroma collection\n",
        "def load_chroma_collection(path: str, name: str):\n",
        "    chroma_client = chromadb.PersistentClient(path=path)\n",
        "    return chroma_client.get_collection(name=name, embedding_function=GeminiEmbeddingFunction())\n"
      ],
      "metadata": {
        "id": "6hPp5hzuATEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db = load_chroma_collection(db_path, db_name)"
      ],
      "metadata": {
        "id": "H67mGvIFCaGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Retrieve the context"
      ],
      "metadata": {
        "id": "U5nmHkKtJIQY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve the most relevant passages based on the query\n",
        "def get_relevant_passage(query: str, db, n_results: int):\n",
        "    results = db.query(query_texts=[query], n_results=n_results)\n",
        "    return [doc[0] for doc in results['documents']]\n"
      ],
      "metadata": {
        "id": "QZgAkHnSCdyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is the AI Maturity Scale?\"\n",
        "relevant_text = get_relevant_passage(query, db, n_results=1)\n"
      ],
      "metadata": {
        "id": "vF80bJ51CjcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Answer Generation"
      ],
      "metadata": {
        "id": "8eZBcrNhJTNe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct a prompt for the generation model based on the query and retrieved data\n",
        "def make_rag_prompt(query: str, relevant_passage: str):\n",
        "    escaped_passage = relevant_passage.replace(\"'\", \"\").replace('\"', \"\").replace(\"\\n\", \" \")\n",
        "    prompt = f\"\"\"You are a helpful and informative bot that answers questions using text from the reference passage included below.\n",
        "Be sure to respond in a complete sentence, being comprehensive, including all relevant background information.\n",
        "However, you are talking to a non-technical audience, so be sure to break down complicated concepts and\n",
        "strike a friendly and conversational tone.\n",
        "QUESTION: '{query}'\n",
        "PASSAGE: '{escaped_passage}'\n",
        "\n",
        "ANSWER:\n",
        "\"\"\"\n",
        "    return prompt\n"
      ],
      "metadata": {
        "id": "gHIcTcG7CnCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate an answer using the Gemini Pro API\n",
        "def generate_answer(prompt: str):\n",
        "\n",
        "    genai.configure(api_key=API_KEY)\n",
        "    model = genai.GenerativeModel('gemini-2.5-flash')\n",
        "    result = model.generate_content(prompt)\n",
        "    return result.text"
      ],
      "metadata": {
        "id": "6aO2CCYUCuRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct the prompt and generate the answer\n",
        "final_prompt = make_rag_prompt(query, \"\".join(relevant_text))\n",
        "answer = generate_answer(final_prompt)\n",
        "print(answer)"
      ],
      "metadata": {
        "id": "0io1DrGGC170"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing"
      ],
      "metadata": {
        "id": "AshUF7ZMJab9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Interactive function to process user input and generate an answer\n",
        "def process_query_and_generate_answer():\n",
        "    query = input(\"Please enter your query: \")\n",
        "    if not query:\n",
        "        print(\"No query provided.\")\n",
        "        return\n",
        "    db = load_chroma_collection(db_path, db_name)\n",
        "    relevant_text = get_relevant_passage(query, db, n_results=1)\n",
        "    if not relevant_text:\n",
        "        print(\"No relevant information found for the given query.\")\n",
        "        return\n",
        "    final_prompt = make_rag_prompt(query, \"\".join(relevant_text))\n",
        "    answer = generate_answer(final_prompt)\n",
        "    print(\"Generated Answer:\", answer)\n"
      ],
      "metadata": {
        "id": "Vmlxm0hsDH0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Invoke the function to interact with user\n",
        "process_query_and_generate_answer()"
      ],
      "metadata": {
        "id": "9NXv6ZltDbsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FUxGiOLZDeDq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}